{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T01:05:30.241165Z",
     "start_time": "2025-12-16T01:05:27.361484Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "from typing import List, Dict, Any\n",
    "from tqdm import tqdm\n",
    "\n",
    "from llm_client import get_llama\n",
    "from llm_prompts import VACANCY_COMPETENCIES_PROMPT\n",
    "\n",
    "import gc as py_gc\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from llm_client import MODEL_NAME"
   ],
   "id": "4c8012b9c9514eac",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Обработка вакансий моделью без QLoRA ",
   "id": "b92c74b618200283"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T23:08:20.180995Z",
     "start_time": "2025-12-15T23:08:10.827192Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# src/analyze_vacancies_llm.py\n",
    "BATCH_SIZE = 7\n",
    "MAX_NEW_TOKENS = 128\n",
    "TRACE_PATH = \"vac_test.txt\"\n",
    "\n",
    "\n",
    "def load_vacancies(path: str) -> List[Dict[str, Any]]:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def _build_prompt(vac: Dict[str, Any]) -> str:\n",
    "    industry = vac.get(\"industry\")\n",
    "    title = vac.get(\"title\")\n",
    "    description = vac.get(\"description\") or \"\"\n",
    "    skills_extracted = vac.get(\"skills_extracted\") or []\n",
    "\n",
    "    return VACANCY_COMPETENCIES_PROMPT.format(\n",
    "        industry=industry,\n",
    "        title=title,\n",
    "        description=description[:4000],  # safety-ограничение\n",
    "        skills_extracted=skills_extracted,\n",
    "    )\n",
    "\n",
    "\n",
    "def _write_trace(f, meta: Dict[str, Any], prompt: str, answer: str) -> None:\n",
    "    f.write(\"\\n\" + \"=\" * 120 + \"\\n\")\n",
    "    f.write(f\"vacancy_id: {meta.get('vacancy_id')}\\n\")\n",
    "    f.write(f\"industry: {meta.get('industry')}\\n\")\n",
    "    f.write(f\"title: {meta.get('title')}\\n\")\n",
    "    f.write(\"-\" * 120 + \"\\n\")\n",
    "    f.write(\"PROMPT:\\n\")\n",
    "    f.write(prompt.rstrip() + \"\\n\")\n",
    "    f.write(\"-\" * 120 + \"\\n\")\n",
    "    f.write(\"MODEL_OUTPUT:\\n\")\n",
    "    f.write((answer or \"\").rstrip() + \"\\n\")\n",
    "    f.write(\"=\" * 120 + \"\\n\")\n",
    "\n",
    "\n",
    "def analyze_vacancies(vacancies_path: str):\n",
    "    llama = get_llama()\n",
    "    try:\n",
    "        vacancies = load_vacancies(vacancies_path)\n",
    "\n",
    "        batch_prompts: List[str] = []\n",
    "        batch_meta: List[Dict[str, Any]] = []\n",
    "\n",
    "        with open(TRACE_PATH, \"w\", encoding=\"utf-8\") as trace_f:\n",
    "            trace_f.write(\"# vac_test.txt — prompts + raw model outputs (one block per vacancy)\\n\")\n",
    "\n",
    "            def process_batch():\n",
    "                nonlocal batch_prompts, batch_meta\n",
    "                if not batch_prompts:\n",
    "                    return\n",
    "\n",
    "                if hasattr(llama, \"ask_batch\"):\n",
    "                    raw_answers = llama.ask_batch(\n",
    "                        batch_prompts,\n",
    "                        max_new_tokens=MAX_NEW_TOKENS,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                    )\n",
    "                else:\n",
    "                    raw_answers = [\n",
    "                        llama.ask_one(p, max_new_tokens=MAX_NEW_TOKENS)\n",
    "                        for p in batch_prompts\n",
    "                    ]\n",
    "\n",
    "                for meta, prompt, raw in zip(batch_meta, batch_prompts, raw_answers):\n",
    "                    _write_trace(trace_f, meta, prompt, raw)\n",
    "\n",
    "                batch_prompts = []\n",
    "                batch_meta = []\n",
    "\n",
    "            for vac in tqdm(vacancies, desc=\"LLM: vacancies (trace to vac_test.txt)\"):\n",
    "                prompt = _build_prompt(vac)\n",
    "\n",
    "                batch_prompts.append(prompt)\n",
    "                batch_meta.append({\n",
    "                    \"vacancy_id\": vac.get(\"id\"),\n",
    "                    \"industry\": vac.get(\"industry\"),\n",
    "                    \"title\": vac.get(\"title\"),\n",
    "                })\n",
    "\n",
    "                if len(batch_prompts) >= BATCH_SIZE:\n",
    "                    process_batch()\n",
    "\n",
    "            process_batch()\n",
    "\n",
    "        print(f\"[OK] Traces saved to {TRACE_PATH}\")\n",
    "\n",
    "    finally:\n",
    "        # <<< ВАЖНО: освобождение VRAM\n",
    "        try:\n",
    "            llama.close()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        from llm_client import reset_llama\n",
    "        reset_llama()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_vacancies(\"data/processed/vacancies_processed.json\")\n"
   ],
   "id": "d987808b1082fb77",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LLM] Проверяем и докачиваем модель (snapshot_download)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/snaw/miniconda3/envs/myenv/lib/python3.12/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Fetching 17 files:   0%|          | 0/17 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4f9de17dc26742598fb7c7d3594eb544"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LLM] Загружаем модель из /home/snaw/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0e476ba23d404521a1a02680f3035d39"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 103\u001B[39m\n\u001B[32m     99\u001B[39m         reset_llama()\n\u001B[32m    102\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[34m__name__\u001B[39m == \u001B[33m\"\u001B[39m\u001B[33m__main__\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m103\u001B[39m     \u001B[43manalyze_vacancies\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mdata/processed/vacancies_processed.json\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 41\u001B[39m, in \u001B[36manalyze_vacancies\u001B[39m\u001B[34m(vacancies_path)\u001B[39m\n\u001B[32m     40\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34manalyze_vacancies\u001B[39m(vacancies_path: \u001B[38;5;28mstr\u001B[39m):\n\u001B[32m---> \u001B[39m\u001B[32m41\u001B[39m     llama = \u001B[43mget_llama\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     42\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m     43\u001B[39m         vacancies = load_vacancies(vacancies_path)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/mnt/c/Users/snaw/PycharmProjects/RH-AI/RH AI/src/llm_client.py:120\u001B[39m, in \u001B[36mget_llama\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m    118\u001B[39m \u001B[38;5;28;01mglobal\u001B[39;00m llama_client\n\u001B[32m    119\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m llama_client \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m120\u001B[39m     llama_client = \u001B[43mLlamaClient\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    121\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m llama_client\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/mnt/c/Users/snaw/PycharmProjects/RH-AI/RH AI/src/llm_client.py:38\u001B[39m, in \u001B[36mLlamaClient.__init__\u001B[39m\u001B[34m(self, device, adapter_dir)\u001B[39m\n\u001B[32m     35\u001B[39m     \u001B[38;5;28mself\u001B[39m.tokenizer.pad_token = \u001B[38;5;28mself\u001B[39m.tokenizer.eos_token\n\u001B[32m     36\u001B[39m     \u001B[38;5;28mself\u001B[39m.tokenizer.pad_token_id = \u001B[38;5;28mself\u001B[39m.tokenizer.eos_token_id\n\u001B[32m---> \u001B[39m\u001B[32m38\u001B[39m \u001B[38;5;28mself\u001B[39m.model = \u001B[43mAutoModelForCausalLM\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     39\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     40\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mauto\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     41\u001B[39m \u001B[43m    \u001B[49m\u001B[43mquantization_config\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbnb_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     42\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtorch_dtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfloat16\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     43\u001B[39m \u001B[43m    \u001B[49m\u001B[43mattn_implementation\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43msdpa\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     44\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     46\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m adapter_dir:\n\u001B[32m     47\u001B[39m     \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpeft\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m PeftModel\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:604\u001B[39m, in \u001B[36m_BaseAutoModelClass.from_pretrained\u001B[39m\u001B[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[39m\n\u001B[32m    602\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m model_class.config_class == config.sub_configs.get(\u001B[33m\"\u001B[39m\u001B[33mtext_config\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[32m    603\u001B[39m         config = config.get_text_config()\n\u001B[32m--> \u001B[39m\u001B[32m604\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodel_class\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    605\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43mmodel_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mhub_kwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    606\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    607\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m    608\u001B[39m     \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mUnrecognized configuration class \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig.\u001B[34m__class__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m for this kind of AutoModel: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m.\u001B[34m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m    609\u001B[39m     \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mModel type should be one of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m.join(c.\u001B[34m__name__\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mc\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28mcls\u001B[39m._model_mapping)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    610\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/transformers/modeling_utils.py:277\u001B[39m, in \u001B[36mrestore_default_dtype.<locals>._wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    275\u001B[39m old_dtype = torch.get_default_dtype()\n\u001B[32m    276\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m277\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    278\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    279\u001B[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/transformers/modeling_utils.py:5048\u001B[39m, in \u001B[36mPreTrainedModel.from_pretrained\u001B[39m\u001B[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001B[39m\n\u001B[32m   5038\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m dtype_orig \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   5039\u001B[39m         torch.set_default_dtype(dtype_orig)\n\u001B[32m   5041\u001B[39m     (\n\u001B[32m   5042\u001B[39m         model,\n\u001B[32m   5043\u001B[39m         missing_keys,\n\u001B[32m   5044\u001B[39m         unexpected_keys,\n\u001B[32m   5045\u001B[39m         mismatched_keys,\n\u001B[32m   5046\u001B[39m         offload_index,\n\u001B[32m   5047\u001B[39m         error_msgs,\n\u001B[32m-> \u001B[39m\u001B[32m5048\u001B[39m     ) = \u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_load_pretrained_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   5049\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5050\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstate_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5051\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcheckpoint_files\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5052\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5053\u001B[39m \u001B[43m        \u001B[49m\u001B[43mignore_mismatched_sizes\u001B[49m\u001B[43m=\u001B[49m\u001B[43mignore_mismatched_sizes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5054\u001B[39m \u001B[43m        \u001B[49m\u001B[43msharded_metadata\u001B[49m\u001B[43m=\u001B[49m\u001B[43msharded_metadata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5055\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5056\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdisk_offload_folder\u001B[49m\u001B[43m=\u001B[49m\u001B[43moffload_folder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5057\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5058\u001B[39m \u001B[43m        \u001B[49m\u001B[43mhf_quantizer\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhf_quantizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5059\u001B[39m \u001B[43m        \u001B[49m\u001B[43mkeep_in_fp32_regex\u001B[49m\u001B[43m=\u001B[49m\u001B[43mkeep_in_fp32_regex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5060\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdevice_mesh\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdevice_mesh\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5061\u001B[39m \u001B[43m        \u001B[49m\u001B[43mkey_mapping\u001B[49m\u001B[43m=\u001B[49m\u001B[43mkey_mapping\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5062\u001B[39m \u001B[43m        \u001B[49m\u001B[43mweights_only\u001B[49m\u001B[43m=\u001B[49m\u001B[43mweights_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5063\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   5064\u001B[39m \u001B[38;5;66;03m# make sure token embedding weights are still tied if needed\u001B[39;00m\n\u001B[32m   5065\u001B[39m model.tie_weights()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/transformers/modeling_utils.py:5468\u001B[39m, in \u001B[36mPreTrainedModel._load_pretrained_model\u001B[39m\u001B[34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001B[39m\n\u001B[32m   5465\u001B[39m         args_list = logging.tqdm(args_list, desc=\u001B[33m\"\u001B[39m\u001B[33mLoading checkpoint shards\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m   5467\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m args \u001B[38;5;129;01min\u001B[39;00m args_list:\n\u001B[32m-> \u001B[39m\u001B[32m5468\u001B[39m         _error_msgs, disk_offload_index = \u001B[43mload_shard_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   5469\u001B[39m         error_msgs += _error_msgs\n\u001B[32m   5471\u001B[39m \u001B[38;5;66;03m# Save offloaded index if needed\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/transformers/modeling_utils.py:843\u001B[39m, in \u001B[36mload_shard_file\u001B[39m\u001B[34m(args)\u001B[39m\n\u001B[32m    841\u001B[39m \u001B[38;5;66;03m# Skip it with fsdp on ranks other than 0\u001B[39;00m\n\u001B[32m    842\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (is_fsdp_enabled() \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_local_dist_rank_0() \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_quantized):\n\u001B[32m--> \u001B[39m\u001B[32m843\u001B[39m     disk_offload_index = \u001B[43m_load_state_dict_into_meta_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    844\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    845\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstate_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    846\u001B[39m \u001B[43m        \u001B[49m\u001B[43mshard_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    847\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreverse_key_renaming_mapping\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    848\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    849\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdisk_offload_folder\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdisk_offload_folder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    850\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdisk_offload_index\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdisk_offload_index\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    851\u001B[39m \u001B[43m        \u001B[49m\u001B[43mhf_quantizer\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhf_quantizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    852\u001B[39m \u001B[43m        \u001B[49m\u001B[43mkeep_in_fp32_regex\u001B[49m\u001B[43m=\u001B[49m\u001B[43mkeep_in_fp32_regex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    853\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdevice_mesh\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdevice_mesh\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    854\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    856\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m error_msgs, disk_offload_index\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001B[39m, in \u001B[36mcontext_decorator.<locals>.decorate_context\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    117\u001B[39m \u001B[38;5;129m@functools\u001B[39m.wraps(func)\n\u001B[32m    118\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdecorate_context\u001B[39m(*args, **kwargs):\n\u001B[32m    119\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[32m--> \u001B[39m\u001B[32m120\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/transformers/modeling_utils.py:750\u001B[39m, in \u001B[36m_load_state_dict_into_meta_model\u001B[39m\u001B[34m(model, state_dict, shard_file, reverse_renaming_mapping, device_map, disk_offload_folder, disk_offload_index, hf_quantizer, keep_in_fp32_regex, device_mesh)\u001B[39m\n\u001B[32m    748\u001B[39m param = param[...]\n\u001B[32m    749\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m casting_dtype \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m750\u001B[39m     param = \u001B[43mparam\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasting_dtype\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    751\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m to_contiguous:\n\u001B[32m    752\u001B[39m     param = param.contiguous()\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Очистка памяти",
   "id": "1f8b060df1c6d735"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T00:51:07.031884Z",
     "start_time": "2025-12-16T00:51:06.301690Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ===== GPU / LLM MEMORY CLEANUP =====\n",
    "\"\"\"\n",
    "эффективнее будет\n",
    "nvidia-smi | grep python\n",
    "pkill -9 python\n",
    "но это убьет вообще все py процессы\n",
    "\"\"\"\n",
    "def cleanup_llm_memory():\n",
    "    try:\n",
    "        import llm_client\n",
    "\n",
    "        # если llama_client существует — закрываем модель\n",
    "        llama = getattr(llm_client, \"llama_client\", None)\n",
    "        if llama is not None:\n",
    "            try:\n",
    "                llama.close()\n",
    "                print(\"[cleanup] llama.close() called\")\n",
    "            except Exception as e:\n",
    "                print(\"[cleanup] llama.close() failed:\", e)\n",
    "\n",
    "        # сбрасываем singleton\n",
    "        if hasattr(llm_client, \"reset_llama\"):\n",
    "            llm_client.reset_llama()\n",
    "            print(\"[cleanup] llama_client reset\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"[cleanup] llm_client not found or already cleaned:\", e)\n",
    "\n",
    "    for name in [\"model\", \"base\", \"pipe\", \"trainer\", \"optimizer\", \"tokenizer\"]:\n",
    "        if name in globals():\n",
    "            del globals()[name]\n",
    "    \n",
    "    py_gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "    \n",
    "    print(\"allocated:\", torch.cuda.memory_allocated()/1024**2, \"MB\")\n",
    "    print(\"reserved :\", torch.cuda.memory_reserved()/1024**2, \"MB\")\n",
    "\n",
    "    print(\"[cleanup] DONE\")\n",
    "\n",
    "cleanup_llm_memory()\n"
   ],
   "id": "5f12ed2b597dc45c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cleanup] llama_client reset\n",
      "allocated: 5241.6845703125 MB\n",
      "reserved : 9006.0 MB\n",
      "[cleanup] DONE\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T00:51:34.391984Z",
     "start_time": "2025-12-16T00:51:34.389643Z"
    }
   },
   "cell_type": "code",
   "source": "## Конфиг и загрузка датасета JSONL",
   "id": "4d853b165c467e1",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T01:05:34.245021Z",
     "start_time": "2025-12-16T01:05:33.700808Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DATA_PATH = \"vac_qloRA_train_v2.jsonl\"   \n",
    "OUT_DIR = \"QLoRA/vac_qlora_adapter\"\n",
    "\n",
    "# Загружаем JSONL как dataset\n",
    "ds = load_dataset(\"json\", data_files=DATA_PATH, split=\"train\")\n",
    "\n",
    "# Быстрый sanity-check\n",
    "print(ds)\n",
    "print(ds[0].keys())\n",
    "print(\"пример prompt:\", ds[0][\"prompt\"][:200].replace(\"\\n\",\" \") + \" ...\")\n",
    "print(\"пример response:\", ds[0][\"response\"])\n"
   ],
   "id": "5576368f72d01c17",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['prompt', 'response'],\n",
      "    num_rows: 311\n",
      "})\n",
      "dict_keys(['prompt', 'response'])\n",
      "пример prompt: Проанализируй текст вакансии и выдели ТОЛЬКО профессиональные компетенции из текста вакансии. Возможными компетенциями могут быть: языки программирования, фреймворки, библиотеки, базы данных и методы  ...\n",
      "пример response: [\"разработка на 1С\", \"сопровождение 1С\", \"написание ТЗ\", \"интеграции\", \"SQL\", \"тестирование доработок\", \"консультация пользователей\"]\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Сплит train/val и сборка текста для SFT",
   "id": "3f89e1adade220c1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T01:05:37.879018Z",
     "start_time": "2025-12-16T01:05:37.871265Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# train/val split (можешь поменять test_size)\n",
    "ds = ds.train_test_split(test_size=0.05, seed=42)\n",
    "train_ds, eval_ds = ds[\"train\"], ds[\"test\"]\n",
    "\n",
    "def format_example(ex):\n",
    "    # response у тебя уже строка с JSON-массивом: '[\"...\",\"...\"]'\n",
    "    # важно добавить EOS, чтобы модель понимала конец\n",
    "    text = ex[\"prompt\"].strip() + \"\\n\\n\" + ex[\"response\"].strip()\n",
    "    return {\"text\": text}\n",
    "\n",
    "train_ds = train_ds.map(format_example, remove_columns=train_ds.column_names)\n",
    "eval_ds  = eval_ds.map(format_example, remove_columns=eval_ds.column_names)\n",
    "\n",
    "print(train_ds[0][\"text\"][:500], \"...\\n\")\n"
   ],
   "id": "bd9d597d31a7688",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Проанализируй текст вакансии и выдели ТОЛЬКО профессиональные компетенции из текста вакансии.\n",
      "Возможными компетенциями могут быть: языки программирования, фреймворки, библиотеки, базы данных и методы работы с ними, разработка чат-ботов, \n",
      "направления в индустриях, например построение ML-моделей, NLP, LLM, а также A/B-тестирование, геймдизайн, Power BI, разработка онлайн-курсов.\n",
      "\n",
      "Данные вакансии:\n",
      "Отрасль: EdTech\n",
      "Название: Оператор онлайн-чата\n",
      "Описание: Работа в офисе !!!!!\n",
      "Обязанности\n",
      "* Ответы на  ...\n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T01:05:41.885702Z",
     "start_time": "2025-12-16T01:05:41.807927Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, pipeline\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n",
    "from trl import SFTTrainer"
   ],
   "id": "eb1a59e13565f4c4",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Конфиг",
   "id": "706cb54b37d389c6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T01:05:44.862520Z",
     "start_time": "2025-12-16T01:05:43.686767Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    use_fast=True,\n",
    ")"
   ],
   "id": "48d2c5b537e1fbff",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Загрузка модели (4-bit QLoRA) ",
   "id": "bc2a08035f67024c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T00:52:12.662679Z",
     "start_time": "2025-12-16T00:51:46.801934Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# pad_token нужен для батчинга\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n"
   ],
   "id": "b9dfb6f4e9432f53",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4840fb8875e24cb4835cd960f6ee5bac"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Обучение (SFTTrainer) и сохранение адаптера",
   "id": "781559e65ecaa34"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T00:52:15.131702Z",
     "start_time": "2025-12-16T00:52:15.128978Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import trl # разные версии работают абсолютно по-разному\n",
    "print(trl.__version__)"
   ],
   "id": "b444d0c0e47afed4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26.1\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T01:05:12.030499200Z",
     "start_time": "2025-12-16T00:52:16.334195Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "# LoRA config (классический QLoRA сетап)\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "args = SFTConfig(\n",
    "    output_dir=OUT_DIR,\n",
    "    max_length=2048,\n",
    "    packing=False,\n",
    "    num_train_epochs=8,\n",
    "    learning_rate=1e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.05,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=3,\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_steps=50,\n",
    "    save_total_limit=5,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    report_to=\"none\",\n",
    "    logging_strategy=\"steps\",   \n",
    "    logging_first_step=True,    \n",
    "    completion_only_loss=True,\n",
    "    bf16=False,\n",
    "    fp16=True,\n",
    "\n",
    "    # КЛЮЧЕВОЕ: отключаем gradient clipping, иначе accelerate вызовет unscale_gradients()\n",
    "    max_grad_norm=0.0,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "# КЛЮЧЕВОЕ: отключаем GradScaler\n",
    "if hasattr(trainer, \"accelerator\") and hasattr(trainer.accelerator, \"scaler\"):\n",
    "    trainer.accelerator.scaler = None\n",
    "    print(\"[patch] GradScaler disabled\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.model.save_pretrained(OUT_DIR)\n",
    "tokenizer.save_pretrained(OUT_DIR)\n",
    "\n",
    "print(f\"[OK] Adapter saved to: {OUT_DIR}\")\n"
   ],
   "id": "8e01052e5f139752",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 83,886,080 || all params: 8,114,147,328 || trainable%: 1.0338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 128001}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[patch] GradScaler disabled\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='256' max='792' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [256/792 12:48 < 27:01, 0.33 it/s, Epoch 2.58/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.018700</td>\n",
       "      <td>0.962100</td>\n",
       "      <td>1.053691</td>\n",
       "      <td>130373.000000</td>\n",
       "      <td>0.782306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.726700</td>\n",
       "      <td>0.746448</td>\n",
       "      <td>0.827214</td>\n",
       "      <td>250001.000000</td>\n",
       "      <td>0.829311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.496900</td>\n",
       "      <td>0.710210</td>\n",
       "      <td>0.657830</td>\n",
       "      <td>373966.000000</td>\n",
       "      <td>0.841228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.427600</td>\n",
       "      <td>0.623209</td>\n",
       "      <td>0.592642</td>\n",
       "      <td>501593.000000</td>\n",
       "      <td>0.858397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.386600</td>\n",
       "      <td>0.641123</td>\n",
       "      <td>0.535451</td>\n",
       "      <td>623666.000000</td>\n",
       "      <td>0.859420</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[9]\u001B[39m\u001B[32m, line 56\u001B[39m\n\u001B[32m     53\u001B[39m     trainer.accelerator.scaler = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m     54\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33m[patch] GradScaler disabled\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m56\u001B[39m \u001B[43mtrainer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     58\u001B[39m trainer.model.save_pretrained(OUT_DIR)\n\u001B[32m     59\u001B[39m tokenizer.save_pretrained(OUT_DIR)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/transformers/trainer.py:2325\u001B[39m, in \u001B[36mTrainer.train\u001B[39m\u001B[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[39m\n\u001B[32m   2323\u001B[39m         hf_hub_utils.enable_progress_bars()\n\u001B[32m   2324\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m2325\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2326\u001B[39m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[43m=\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2327\u001B[39m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m=\u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2328\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2329\u001B[39m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m=\u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2330\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/transformers/trainer.py:2674\u001B[39m, in \u001B[36mTrainer._inner_training_loop\u001B[39m\u001B[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[39m\n\u001B[32m   2667\u001B[39m context = (\n\u001B[32m   2668\u001B[39m     functools.partial(\u001B[38;5;28mself\u001B[39m.accelerator.no_sync, model=model)\n\u001B[32m   2669\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m i != \u001B[38;5;28mlen\u001B[39m(batch_samples) - \u001B[32m1\u001B[39m\n\u001B[32m   2670\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001B[32m   2671\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m contextlib.nullcontext\n\u001B[32m   2672\u001B[39m )\n\u001B[32m   2673\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m context():\n\u001B[32m-> \u001B[39m\u001B[32m2674\u001B[39m     tr_loss_step = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_items_in_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2676\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m   2677\u001B[39m     args.logging_nan_inf_filter\n\u001B[32m   2678\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_xla_available()\n\u001B[32m   2679\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m (torch.isnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch.isinf(tr_loss_step))\n\u001B[32m   2680\u001B[39m ):\n\u001B[32m   2681\u001B[39m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[32m   2682\u001B[39m     tr_loss = tr_loss + tr_loss / (\u001B[32m1\u001B[39m + \u001B[38;5;28mself\u001B[39m.state.global_step - \u001B[38;5;28mself\u001B[39m._globalstep_last_logged)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:1245\u001B[39m, in \u001B[36mSFTTrainer.training_step\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1243\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mtraining_step\u001B[39m(\u001B[38;5;28mself\u001B[39m, *args, **kwargs):\n\u001B[32m   1244\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m.maybe_activation_offload_context:\n\u001B[32m-> \u001B[39m\u001B[32m1245\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/transformers/trainer.py:4071\u001B[39m, in \u001B[36mTrainer.training_step\u001B[39m\u001B[34m(***failed resolving arguments***)\u001B[39m\n\u001B[32m   4068\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.accelerator.distributed_type == DistributedType.DEEPSPEED:\n\u001B[32m   4069\u001B[39m         kwargs[\u001B[33m\"\u001B[39m\u001B[33mscale_wrt_gas\u001B[39m\u001B[33m\"\u001B[39m] = \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m4071\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43maccelerator\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mloss\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   4073\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m loss.detach()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/accelerate/accelerator.py:2740\u001B[39m, in \u001B[36mAccelerator.backward\u001B[39m\u001B[34m(self, loss, **kwargs)\u001B[39m\n\u001B[32m   2738\u001B[39m     \u001B[38;5;28mself\u001B[39m.lomo_backward(loss, learning_rate)\n\u001B[32m   2739\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m2740\u001B[39m     \u001B[43mloss\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/torch/_tensor.py:625\u001B[39m, in \u001B[36mTensor.backward\u001B[39m\u001B[34m(self, gradient, retain_graph, create_graph, inputs)\u001B[39m\n\u001B[32m    615\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    616\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[32m    617\u001B[39m         Tensor.backward,\n\u001B[32m    618\u001B[39m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[32m   (...)\u001B[39m\u001B[32m    623\u001B[39m         inputs=inputs,\n\u001B[32m    624\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m625\u001B[39m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mautograd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    626\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs\u001B[49m\n\u001B[32m    627\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001B[39m, in \u001B[36mbackward\u001B[39m\u001B[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[39m\n\u001B[32m    349\u001B[39m     retain_graph = create_graph\n\u001B[32m    351\u001B[39m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[32m    352\u001B[39m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[32m    353\u001B[39m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m354\u001B[39m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    355\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    356\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    357\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    358\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    359\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs_tuple\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    360\u001B[39m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    361\u001B[39m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    362\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/torch/autograd/graph.py:841\u001B[39m, in \u001B[36m_engine_run_backward\u001B[39m\u001B[34m(t_outputs, *args, **kwargs)\u001B[39m\n\u001B[32m    839\u001B[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[32m    840\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m841\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_execution_engine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[32m    842\u001B[39m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    843\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[32m    844\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    845\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Быстрый тест",
   "id": "7a490b1865eaff63"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### выгрузка модели",
   "id": "465ce4242b71ec04"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T23:29:04.912867Z",
     "start_time": "2025-12-15T23:28:37.046324Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "from peft import PeftModel\n",
    "\n",
    "MODEL_NAME = \"meta-llama/Llama-3.1-8B\"\n",
    "ADAPTER_DIR = OUT_DIR  # или r\"RH-AI\\RH AI\\src\\QLoRA\\vac_qlora_adapter\"\n",
    "\n",
    "# 4-bit config (как в обучении)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# базовая модель целиком на GPU:0 (без CPU/disk dispatch)\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": 0},\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# подключаем LoRA адаптер\n",
    "model = PeftModel.from_pretrained(base, ADAPTER_DIR)\n",
    "model.eval()\n",
    "\n",
    "# генератор\n",
    "gen_pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map={\"\": 0},\n",
    "    return_full_text=False,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    ")\n",
    "\n",
    "print(\"[OK] base+LoRA loaded, gen_pipe is ready\")\n"
   ],
   "id": "e8d35fdcb57a7d3b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ac8102b419504748a96edae305f16cbc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] base+LoRA loaded, gen_pipe is ready\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Сам тест",
   "id": "949232d34353f9a2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T23:45:25.431797Z",
     "start_time": "2025-12-15T23:45:24.200126Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "TEST_PROMPT = \"\"\"Проанализируй текст вакансии и выдели ТОЛЬКО профессиональные компетенции из текста вакансии.\n",
    "Возможными компетенциями могут быть: языки программирования, фреймворки, библиотеки, базы данных и методы работы с ними, разработка чат-ботов, \n",
    "направления в индустриях, например построение ML-моделей, NLP, LLM, а также A/B-тестирование, геймдизайн, Power BI, разработка онлайн-курсов.\n",
    "\n",
    "Данные вакансии:\n",
    "Отрасль: 1CDevelopment\n",
    "Название: IT-специалист / Инженер 1 линии технической поддержки\n",
    "Описание: Описание вакансии\n",
    "Компания \"РОСТ\" - системный интегратор в области информационных технологий и Партнёр ведущих российских разработчиков Программного обеспечения, таких как: СБИС; 1С Bitrix; LiteBox и другие.\n",
    "Обязанности:\n",
    "• Принимать «горячие» обращения от клиентов\n",
    "• преимущественно звонки, а также мессенджеры;\n",
    "• правильно оформлять и заносить заявки в Help Desk;\n",
    "• удаленно оперативно устранять сбои в работе программно-аппаратного комплекса клиента\n",
    "• (система для автоматизации предприятий.);\n",
    "• правильно настраивать сеть, торговое оборудование и компьютерную технику;\n",
    "• качественно консультировать и обучать клиентов;\n",
    "• быть готовым работать с конфликтными клиентами;\n",
    "• уметь работать в команде.\n",
    "• выезд, если нет возможности решить проблему удаленно;\n",
    "• ведение и поддержание в актуальном состоянии документации;\n",
    "Требования:\n",
    "• опыт работы системным администратором или инженером технической поддержки (helpdesk/servicedesk) приветствуется;\n",
    "• опыт решения пользовательских задач (проблемы с офисным ПО, оргтехникой, работа с общими папками, сетевая печать, ЭЦП и т. п.) приветствуется;\n",
    "• опыт работы с антивирусным ПО (в т. ч. с централизованной консолью управления (в основном мы используем Kaspersky, Dr. Web);\n",
    "• плюсом будет наличие сертификатов (в области\n",
    "• ИТ\n",
    "• плюсом будет опыт администрирования 1С;\n",
    "• плюсом будет опыт работы в Битрикс24;\n",
    "...\n",
    "Формат ответа:\n",
    "[\"компетенция1\", \"компетенция2\", ...]\"\"\"\n",
    "\n",
    "test2 = \"\"\"Проанализируй текст вакансии и выдели ТОЛЬКО профессиональные компетенции из текста вакансии.\n",
    "Возможными компетенциями могут быть: языки программирования, фреймворки, библиотеки, базы данных и методы работы с ними, разработка чат-ботов, \n",
    "направления в индустриях, например построение ML-моделей, NLP, LLM, а также A/B-тестирование, геймдизайн, Power BI, разработка онлайн-курсов.\n",
    "\n",
    "Данные вакансии:\n",
    "Отрасль: ChatBots\n",
    "Название: Продавец-кассир (ТЦ Макси Сопот, Приморская)\n",
    "Описание: Familia - основоположник и лидер российского off-price ритейла - благодаря своей товарной и ценовой политике, широте и частоте обновление ассортимента. В магазинах сети представлен широкий ассортимент мужской, женской и детской одежды и обуви, аксессуары, игрушки, весь спектр товаров для дома и декора, товары для домашних питомцев и многое другое по максимально выгодным ценам.\n",
    "Уважаемые кандидаты, мы ждём Вас на собеседование в магазине Familia со вторника по субботу с 10:00 до 19:00 БЕЗ ПРЕДВАРИТЕЛЬНОЙ ЗАПИСИ.\n",
    "По дополнительным вопросам, пожалуйста, звоните по указанному номеру телефона или пишите в чат, WhatsApp.\n",
    "Мы предлагаем:\n",
    "• Оформление по ТК РФ с первого дня работы, оплачиваемый больничный лист, отпуск.\n",
    "• Официальная белая заработная плата, выплачивается два раза в месяц.\n",
    "• Стабильный оклад + ежемесячная премия 15% по итогам работы магазина.\n",
    "• Отсутствие личных продаж!\n",
    "• График работы 2/2.\n",
    "• Предоставление форменной одежды.\n",
    "• Корпоративное обучение.\n",
    "• Перспектива карьерного роста до Заместителя управляющего магазином, Управляющего магазином.\n",
    "• Бонус по акции \"Приведи друга\" до 5000 руб.\n",
    "• Программа лояльности для сотрудников по системе кэшбек баллов.\n",
    "• Новогодние подарки для детей сотрудников.\n",
    "• Дополнительные конкурсные программы с денежным вознаграждением.\n",
    "• Бонусные программы для сотрудников (скидки от партнеров: красота, спорт, развлечения, электроника, туризм и многое другое).\n",
    "Основные задачи:\n",
    "• Работа за кассовым терминалом, обслуживание покупателей на кассе.\n",
    "• Поддержание порядка, соблюдение стандартов визуального мерчендайзинга и выкладка товара в прикассовой зоне\n",
    "• Участие в инвентаризации (1 раз в год).\n",
    "Для нас важно:\n",
    "• Активность и готовность обучаться.\n",
    "• Ответственность, внимательность, доброжелательность.\n",
    "• Имеете опыт работы – отлично! Без опыта – обучим!\n",
    "Приветствуется опыт работы на вакансиях: продавец, без опыта, менеджер по продажам, консультант, кассир, менеджер по работе с клиентами, начинающий специалист, ртз, работник торгового зала, кассир с ежедневными выплатами в компаниях:\n",
    "Gloria Jeans, Глория Джинс, Sela, Zarina, Зарина, Детский мир, Дочки-сыночки, Defacto, LC Waikiki, Sinsay, Sin, Синсей, Reserved, H&M, Cropp, House, Modis, Модис, Decatlon, Декатлон, MAAG, ECRU, DUB, VILET, Новая мода, Zara, Gloria-Jeans, GJ, Зара, Incity, Инсити, Inditex, Bershka, Massimo Dutti, Uterque, Pull&Bear, Stradivarius, Oysho, Lefties, Ойшо, Бершка, Пулл энд Беар, Массимо Дутти, Лефтиес, Kari, Кари, Intimissimi, Calzedonia, Кальзедония, Incanto, Calvin Klein, Collins, Коллинс, ТВОЕ, Levi’s, Mango, 12Storeez, Подружка, 5 карманов, Koton, Котон, Love Republic, New Yorker, Terranova, Терранова, Finn Flare, Фин Флэр, Zolla, Золла, Oodji, Оджи, Adidas, МВидео, Леруа Мерлен, OBI, SuperStep, Триал-Спорт, ZENDEN, KIABI, Киаби, RESPECT, TAMARIS, TERVOLINA, Хоум Маркет, Терволина, Nike, PUMA, Columbia, Modis, Demix, Mango, МВидео, Terranova, Эльдорадо, ИКЕА, Zolla, Oodj, Love Republic, Reebok, Связной, МТС, Мегафон, Билайн, Adidas, New Balance, Decathlon, Puma, Reebok, New Balance, Fun Day, Фандэй, Фандей, Sportmaster, Befre, Мохито, Mohito, Benetton, Intimissimi, Интимисими, Calzedonia, Калзедония, Incanto, Инканто, Incity, Инсити, Uniqlo, Юникло, Подружка, SuperStep, Zenden, Kiabi, Киаби, Respect, Rendez-vous, Рандеву, Терволина, МВидео, Эльдорадо, ИКЕА, Леруа Мерлен, OBI, Hoff, Хоф, Твой Дом, Галамарт, Хоум Маркет, DNS, Связной, МТС, Мегафон, Билайн, Вкусно и точка, Макдоналдс, Золотое яблоко, Idol, Yandex, KFC, Бургер Кинг, Burger King, Ростикс, Иль де боте, Fix price, Off price, Сбербанк, Тинькофф, Tinkoff, СДЭК, OZON, WILDBERRIES, Остин, Озон, Валдберрис, Kidzania, Кидзания, Совкомбанк, Альфа-Банк, Додо Пицца, Melon Fashion Group, Apple, Samsung, IL Патио, Шоколадница, Кофе Хаус, Cofix, Stars coffee, Surf Coffee, Gate31, 12 Cторис, Bell you, Джинсовая Симфония, Jeans Symphony, Ривгош, ИвРоше, Tous, Yves Rocher, One&Double, Double coffee, Star Hit, Nice Price, Bubble tea, Братья Караваевы, Гурманика, Азбука Вкуса, Вкусви\n",
    "\n",
    "Верни только JSON-массив строк. Компетенций(строк) в массиве должно быть не менее одной, но НЕ БОЛЕЕ 7. Компетенций должны быть выделены строго из текста вакансии.\n",
    "Не добавляй в массив строк компетенции, не относящиеся к вакансии.\n",
    "Формат ответа:\n",
    "[\"компетенция1\", \"компетенция2\", ...]\"\"\"\n",
    "tokens = tokenizer(TEST_PROMPT, return_tensors=None, add_special_tokens=False)\n",
    "print(\"Tokens:\", len(tokens[\"input_ids\"]))\n",
    "tokens = tokenizer(test2, return_tensors=None, add_special_tokens=False)\n",
    "print(\"Tokens:\", len(tokens[\"input_ids\"]))\n",
    "\"\"\"\n",
    "print(\"=== QLoRA QUICK TEST ===\")\n",
    "with torch.inference_mode():\n",
    "    gen = gen_pipe(\n",
    "        TEST_PROMPT,\n",
    "        max_new_tokens=128,\n",
    "        min_new_tokens=2,\n",
    "        do_sample=True,\n",
    "        temperature=0.3,\n",
    "        top_p=0.9,\n",
    "    )[0][\"generated_text\"]\n",
    "\n",
    "print(gen)\n",
    "\"\"\"\n",
    "\n"
   ],
   "id": "60134d9332101c16",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: 614\n",
      "Tokens: 1814\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nprint(\"=== QLoRA QUICK TEST ===\")\\nwith torch.inference_mode():\\n    gen = gen_pipe(\\n        TEST_PROMPT,\\n        max_new_tokens=128,\\n        min_new_tokens=2,\\n        do_sample=True,\\n        temperature=0.3,\\n        top_p=0.9,\\n    )[0][\"generated_text\"]\\n\\nprint(gen)\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
