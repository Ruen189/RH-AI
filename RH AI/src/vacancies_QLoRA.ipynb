{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T19:44:08.748088Z",
     "start_time": "2025-12-18T19:44:04.738885Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "from typing import List, Dict, Any\n",
    "from tqdm import tqdm\n",
    "\n",
    "from llm_client import get_llama\n",
    "from llm_prompts import VACANCY_COMPETENCIES_PROMPT\n",
    "\n",
    "import gc as py_gc\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from llm_client import MODEL_NAME"
   ],
   "id": "4c8012b9c9514eac",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Обработка вакансий моделью с или без QLoRA",
   "id": "b92c74b618200283"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T14:48:39.556525Z",
     "start_time": "2025-12-18T14:39:40.882685Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# src/analyze_vacancies_llm.py\n",
    "from llm_client import reset_llama\n",
    "BATCH_SIZE = 7\n",
    "MAX_NEW_TOKENS = 128\n",
    "TRACE_PATH = \"vac_test.txt\"\n",
    "\n",
    "def load_vacancies(path: str) -> List[Dict[str, Any]]:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def _build_prompt(vac: Dict[str, Any]) -> str:\n",
    "    industry = vac.get(\"industry\")\n",
    "    title = vac.get(\"title\")\n",
    "    description = vac.get(\"description\") or \"\"\n",
    "    skills_extracted = vac.get(\"skills_extracted\") or []\n",
    "\n",
    "    return VACANCY_COMPETENCIES_PROMPT.format(\n",
    "        industry=industry,\n",
    "        title=title,\n",
    "        description=description[:4000],  # safety-ограничение\n",
    "        skills_extracted=skills_extracted,\n",
    "    )\n",
    "\n",
    "def _write_trace(f, meta: Dict[str, Any], prompt: str, answer: str) -> None:\n",
    "    f.write(\"\\n\" + \"=\" * 120 + \"\\n\")\n",
    "    f.write(f\"vacancy_id: {meta.get('vacancy_id')}\\n\")\n",
    "    f.write(f\"industry: {meta.get('industry')}\\n\")\n",
    "    f.write(f\"title: {meta.get('title')}\\n\")\n",
    "    f.write(\"-\" * 120 + \"\\n\")\n",
    "    f.write(\"PROMPT:\\n\")\n",
    "    f.write(prompt.rstrip() + \"\\n\")\n",
    "    f.write(\"-\" * 120 + \"\\n\")\n",
    "    f.write(\"MODEL_OUTPUT:\\n\")\n",
    "    f.write((answer or \"\").rstrip() + \"\\n\")\n",
    "    f.write(\"=\" * 120 + \"\\n\")\n",
    "\n",
    "def use_qlora(use: bool = True,\n",
    "              adapter: str = r\"QLoRA/vac_qlora_adapter/checkpoint-200\"):\n",
    "    ADAPTER_DIR = None\n",
    "    if use:\n",
    "        ADAPTER_DIR = adapter\n",
    "    return ADAPTER_DIR\n",
    "\n",
    "reset_llama()\n",
    "print(\"Память очищена\")\n",
    "def analyze_vacancies(vacancies_path: str):\n",
    "    adapter = r\"QLoRA/vac_qlora_adapter/checkpoint-200\"  # путь до QLoRA\n",
    "    ADAPTER_DIR = use_qlora(False)\n",
    "    if ADAPTER_DIR:\n",
    "        print(\"adapter_dir:\", ADAPTER_DIR)\n",
    "        print(\"exists:\", os.path.exists(ADAPTER_DIR))\n",
    "        print(\"adapter_config exists:\", os.path.exists(os.path.join(ADAPTER_DIR, \"adapter_config.json\")))\n",
    "    llama = get_llama(adapter_dir=ADAPTER_DIR)\n",
    "    try:\n",
    "        vacancies = load_vacancies(vacancies_path)\n",
    "\n",
    "        batch_prompts: List[str] = []\n",
    "        batch_meta: List[Dict[str, Any]] = []\n",
    "\n",
    "        with open(TRACE_PATH, \"w\", encoding=\"utf-8\") as trace_f:\n",
    "            trace_f.write(\"# vac_test.txt — prompts + raw model outputs (one block per vacancy)\\n\")\n",
    "\n",
    "            def process_batch():\n",
    "                nonlocal batch_prompts, batch_meta\n",
    "                if not batch_prompts:\n",
    "                    return\n",
    "\n",
    "                if hasattr(llama, \"ask_batch\"):\n",
    "                    raw_answers = llama.ask_batch(\n",
    "                        batch_prompts,\n",
    "                        max_new_tokens=MAX_NEW_TOKENS,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                    )\n",
    "                else:\n",
    "                    raw_answers = [\n",
    "                        llama.ask_one(p, max_new_tokens=MAX_NEW_TOKENS)\n",
    "                        for p in batch_prompts\n",
    "                    ]\n",
    "\n",
    "                for meta, prompt, raw in zip(batch_meta, batch_prompts, raw_answers):\n",
    "                    _write_trace(trace_f, meta, prompt, raw)\n",
    "\n",
    "                batch_prompts = []\n",
    "                batch_meta = []\n",
    "\n",
    "            for vac in tqdm(vacancies, desc=\"LLM: vacancies (trace to vac_test.txt)\"):\n",
    "                prompt = _build_prompt(vac)\n",
    "\n",
    "                batch_prompts.append(prompt)\n",
    "                batch_meta.append({\n",
    "                    \"vacancy_id\": vac.get(\"id\"),\n",
    "                    \"industry\": vac.get(\"industry\"),\n",
    "                    \"title\": vac.get(\"title\"),\n",
    "                })\n",
    "\n",
    "                if len(batch_prompts) >= BATCH_SIZE:\n",
    "                    process_batch()\n",
    "\n",
    "            process_batch()\n",
    "\n",
    "        print(f\"[OK] Traces saved to {TRACE_PATH}\")\n",
    "\n",
    "    finally:\n",
    "        try:\n",
    "            llama.close()\n",
    "        except Exception:\n",
    "            pass\n",
    "        del llama  # важно в ноутбуке\n",
    "    \n",
    "        from llm_client import reset_llama\n",
    "        reset_llama()\n",
    "    \n",
    "        import gc, torch\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.ipc_collect()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_vacancies(\"data/processed/vacancies_processed.json\")\n"
   ],
   "id": "d987808b1082fb77",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Память очищена\n",
      "[LLM] Проверяем и докачиваем модель (snapshot_download)...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Fetching 17 files:   0%|          | 0/17 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "564e00fe3dca4b46bde14f8b6d1ca5b1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LLM] Загружаем модель из /home/snaw/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "13e786c0589e413c911b74eda259db93"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LLM] LlamaClient готов к работе.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM: vacancies (trace to vac_test.txt):   0%|          | 0/311 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM: vacancies (trace to vac_test.txt):   2%|▏         | 7/311 [00:15<11:11,  2.21s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM: vacancies (trace to vac_test.txt):   5%|▍         | 14/311 [00:26<08:55,  1.80s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM: vacancies (trace to vac_test.txt):   7%|▋         | 21/311 [00:37<08:17,  1.72s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM: vacancies (trace to vac_test.txt):   9%|▉         | 28/311 [00:52<08:52,  1.88s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM: vacancies (trace to vac_test.txt):  11%|█▏        | 35/311 [01:04<08:19,  1.81s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM: vacancies (trace to vac_test.txt):  14%|█▎        | 42/311 [01:18<08:29,  1.89s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM: vacancies (trace to vac_test.txt):  16%|█▌        | 49/311 [01:31<08:09,  1.87s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM: vacancies (trace to vac_test.txt):  18%|█▊        | 56/311 [01:43<07:44,  1.82s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM: vacancies (trace to vac_test.txt):  20%|██        | 63/311 [02:00<08:25,  2.04s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM: vacancies (trace to vac_test.txt):  23%|██▎       | 70/311 [02:12<07:45,  1.93s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM: vacancies (trace to vac_test.txt):  25%|██▍       | 77/311 [02:29<08:05,  2.08s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM: vacancies (trace to vac_test.txt):  29%|██▉       | 91/311 [02:49<06:19,  1.72s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM: vacancies (trace to vac_test.txt):  32%|███▏      | 98/311 [02:57<05:29,  1.54s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM: vacancies (trace to vac_test.txt):  34%|███▍      | 105/311 [03:12<05:55,  1.72s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM: vacancies (trace to vac_test.txt):  36%|███▌      | 112/311 [03:24<05:44,  1.73s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM: vacancies (trace to vac_test.txt):  38%|███▊      | 119/311 [03:35<05:24,  1.69s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM: vacancies (trace to vac_test.txt):  41%|████      | 126/311 [03:51<05:39,  1.84s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM: vacancies (trace to vac_test.txt):  43%|████▎     | 133/311 [04:02<05:17,  1.79s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM: vacancies (trace to vac_test.txt):  45%|████▌     | 140/311 [04:16<05:15,  1.84s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM: vacancies (trace to vac_test.txt):  47%|████▋     | 147/311 [04:28<04:55,  1.80s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM: vacancies (trace to vac_test.txt):  50%|████▉     | 154/311 [04:39<04:35,  1.75s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM: vacancies (trace to vac_test.txt):  52%|█████▏    | 161/311 [04:51<04:19,  1.73s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM: vacancies (trace to vac_test.txt):  54%|█████▍    | 168/311 [05:04<04:10,  1.75s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM: vacancies (trace to vac_test.txt):  56%|█████▋    | 175/311 [05:15<03:51,  1.70s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM: vacancies (trace to vac_test.txt):  59%|█████▊    | 182/311 [05:31<04:00,  1.86s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM: vacancies (trace to vac_test.txt):  61%|██████    | 189/311 [05:42<03:40,  1.81s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM: vacancies (trace to vac_test.txt):  63%|██████▎   | 196/311 [05:54<03:23,  1.77s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM: vacancies (trace to vac_test.txt):  65%|██████▌   | 203/311 [06:09<03:23,  1.88s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM: vacancies (trace to vac_test.txt):  68%|██████▊   | 210/311 [06:13<02:28,  1.47s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM: vacancies (trace to vac_test.txt):  70%|██████▉   | 217/311 [06:23<02:17,  1.46s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM: vacancies (trace to vac_test.txt):  72%|███████▏  | 224/311 [06:33<02:07,  1.46s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM: vacancies (trace to vac_test.txt):  74%|███████▍  | 231/311 [06:49<02:17,  1.72s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM: vacancies (trace to vac_test.txt):  77%|███████▋  | 238/311 [07:00<02:00,  1.65s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM: vacancies (trace to vac_test.txt):  79%|███████▉  | 245/311 [07:08<01:39,  1.50s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM: vacancies (trace to vac_test.txt):  81%|████████  | 252/311 [07:21<01:35,  1.63s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM: vacancies (trace to vac_test.txt):  83%|████████▎ | 259/311 [07:31<01:21,  1.56s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM: vacancies (trace to vac_test.txt):  86%|████████▌ | 266/311 [07:41<01:07,  1.51s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM: vacancies (trace to vac_test.txt):  88%|████████▊ | 273/311 [07:54<01:01,  1.63s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM: vacancies (trace to vac_test.txt):  90%|█████████ | 280/311 [08:05<00:49,  1.59s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM: vacancies (trace to vac_test.txt):  92%|█████████▏| 287/311 [08:11<00:32,  1.37s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM: vacancies (trace to vac_test.txt):  95%|█████████▍| 294/311 [08:25<00:26,  1.58s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM: vacancies (trace to vac_test.txt):  97%|█████████▋| 301/311 [08:36<00:15,  1.58s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "LLM: vacancies (trace to vac_test.txt): 100%|██████████| 311/311 [08:47<00:00,  1.70s/it]\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Traces saved to vac_test.txt\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Очистка памяти",
   "id": "1f8b060df1c6d735"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T19:45:18.299999Z",
     "start_time": "2025-12-18T19:45:17.668768Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ===== GPU / LLM MEMORY CLEANUP =====\n",
    "\"\"\"\n",
    "эффективнее будет\n",
    "nvidia-smi | grep python\n",
    "pkill -9 python\n",
    "но это убьет вообще все py процессы\n",
    "\"\"\"\n",
    "def cleanup_llm_memory():\n",
    "    try:\n",
    "        import llm_client\n",
    "\n",
    "        # если llama_client существует — закрываем модель\n",
    "        llama = getattr(llm_client, \"llama_client\", None)\n",
    "        if llama is not None:\n",
    "            try:\n",
    "                llama.close()\n",
    "                print(\"[cleanup] llama.close() called\")\n",
    "            except Exception as e:\n",
    "                print(\"[cleanup] llama.close() failed:\", e)\n",
    "\n",
    "        # сбрасываем singleton\n",
    "        if hasattr(llm_client, \"reset_llama\"):\n",
    "            llm_client.reset_llama()\n",
    "            print(\"[cleanup] llama_client reset\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"[cleanup] llm_client not found or already cleaned:\", e)\n",
    "\n",
    "    for name in [\"model\", \"base\", \"pipe\", \"trainer\", \"optimizer\", \"tokenizer\"]:\n",
    "        if name in globals():\n",
    "            del globals()[name]\n",
    "    \n",
    "    import torch\n",
    "    py_gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "    \n",
    "    print(\"allocated:\", torch.cuda.memory_allocated()/1024**2, \"MB\")\n",
    "    print(\"reserved :\", torch.cuda.memory_reserved()/1024**2, \"MB\")\n",
    "\n",
    "    print(\"[cleanup] DONE\")\n",
    "\n",
    "cleanup_llm_memory()\n"
   ],
   "id": "5f12ed2b597dc45c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cleanup] llama_client reset\n",
      "allocated: 0.0009765625 MB\n",
      "reserved : 2.0 MB\n",
      "[cleanup] DONE\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T00:51:34.391984Z",
     "start_time": "2025-12-16T00:51:34.389643Z"
    }
   },
   "cell_type": "code",
   "source": "## Конфиг и загрузка датасета JSONL",
   "id": "4d853b165c467e1",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T19:44:19.821574Z",
     "start_time": "2025-12-18T19:44:19.257952Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DATA_PATH = \"vac_qloRA_train_v2.jsonl\"   \n",
    "OUT_DIR = \"QLoRA/vac_qlora_adapter\"\n",
    "\n",
    "# Загружаем JSONL как dataset\n",
    "ds = load_dataset(\"json\", data_files=DATA_PATH, split=\"train\")\n",
    "\n",
    "# Быстрый sanity-check\n",
    "print(ds)\n",
    "print(ds[0].keys())\n",
    "print(\"пример prompt:\", ds[0][\"prompt\"][:200].replace(\"\\n\",\" \") + \" ...\")\n",
    "print(\"пример response:\", ds[0][\"response\"])\n"
   ],
   "id": "5576368f72d01c17",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "73a84c11d0c24bdb9d91d8464c58a178"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['prompt', 'response'],\n",
      "    num_rows: 311\n",
      "})\n",
      "dict_keys(['prompt', 'response'])\n",
      "пример prompt: Проанализируй текст вакансии и выдели ТОЛЬКО профессиональные компетенции из текста вакансии. Возможными компетенциями могут быть: языки программирования, фреймворки, библиотеки, базы данных и методы  ...\n",
      "пример response: [\"разработка на 1С\", \"сопровождение 1С\", \"написание ТЗ\", \"интеграции\", \"SQL\", \"тестирование доработок\", \"консультация пользователей\"]\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Сплит train/val и сборка текста для SFT",
   "id": "3f89e1adade220c1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T19:44:27.007209Z",
     "start_time": "2025-12-18T19:44:26.962605Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ds = ds.train_test_split(test_size=0.05, seed=42)\n",
    "train_ds, eval_ds = ds[\"train\"], ds[\"test\"]\n",
    "\n",
    "def format_example(ex):\n",
    "\n",
    "    text = ex[\"prompt\"].strip() + \"\\n\\n\" + ex[\"response\"].strip()\n",
    "    return {\"text\": text}\n",
    "\n",
    "train_ds = train_ds.map(format_example, remove_columns=train_ds.column_names)\n",
    "eval_ds  = eval_ds.map(format_example, remove_columns=eval_ds.column_names)\n",
    "\n",
    "print(train_ds[0][\"text\"][:500], \"...\\n\")\n"
   ],
   "id": "bd9d597d31a7688",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/295 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "699f0af8ad0f44ff86ef45149b11b479"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/16 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1097654dd14140658425107b6af5906f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Проанализируй текст вакансии и выдели ТОЛЬКО профессиональные компетенции из текста вакансии.\n",
      "Возможными компетенциями могут быть: языки программирования, фреймворки, библиотеки, базы данных и методы работы с ними, разработка чат-ботов, \n",
      "направления в индустриях, например построение ML-моделей, NLP, LLM, а также A/B-тестирование, геймдизайн, Power BI, разработка онлайн-курсов.\n",
      "\n",
      "Данные вакансии:\n",
      "Отрасль: EdTech\n",
      "Название: Оператор онлайн-чата\n",
      "Описание: Работа в офисе !!!!!\n",
      "Обязанности\n",
      "* Ответы на  ...\n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T19:44:32.556202Z",
     "start_time": "2025-12-18T19:44:32.434776Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, pipeline\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n",
    "from trl import SFTTrainer"
   ],
   "id": "eb1a59e13565f4c4",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Конфиг",
   "id": "706cb54b37d389c6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T19:44:35.133389Z",
     "start_time": "2025-12-18T19:44:34.052522Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    use_fast=True,\n",
    ")"
   ],
   "id": "48d2c5b537e1fbff",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Загрузка модели (4-bit QLoRA) ",
   "id": "bc2a08035f67024c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T19:45:06.560576Z",
     "start_time": "2025-12-18T19:44:37.264141Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# pad_token нужен для батчинга\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n"
   ],
   "id": "b9dfb6f4e9432f53",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7dcdde6845d14ddc9c69dac42de0687b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Обучение (SFTTrainer) и сохранение адаптера",
   "id": "781559e65ecaa34"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T07:44:59.209465Z",
     "start_time": "2025-12-16T07:44:59.204856Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import trl # разные версии работают абсолютно по-разному\n",
    "print(trl.__version__)"
   ],
   "id": "b444d0c0e47afed4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26.1\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T01:05:12.030499200Z",
     "start_time": "2025-12-16T00:52:16.334195Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "# LoRA config (классический QLoRA сетап)\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "args = SFTConfig(\n",
    "    output_dir=OUT_DIR,\n",
    "    max_length=2048, # входящих токенов может быть до 1800\n",
    "    packing=False,\n",
    "    num_train_epochs=8,\n",
    "    learning_rate=1e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.05,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=3,\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_steps=50,\n",
    "    save_total_limit=5,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    report_to=\"none\",\n",
    "    logging_strategy=\"steps\",   \n",
    "    logging_first_step=True,    \n",
    "    completion_only_loss=True,\n",
    "    bf16=False,\n",
    "    fp16=True,\n",
    "    max_grad_norm=0.0,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "# КЛЮЧЕВОЕ: отключаем GradScaler\n",
    "if hasattr(trainer, \"accelerator\") and hasattr(trainer.accelerator, \"scaler\"):\n",
    "    trainer.accelerator.scaler = None\n",
    "    print(\"[patch] GradScaler disabled\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.model.save_pretrained(OUT_DIR)\n",
    "tokenizer.save_pretrained(OUT_DIR)\n",
    "\n",
    "print(f\"[OK] Adapter saved to: {OUT_DIR}\")\n"
   ],
   "id": "8e01052e5f139752",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 83,886,080 || all params: 8,114,147,328 || trainable%: 1.0338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 128001}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[patch] GradScaler disabled\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='256' max='792' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [256/792 12:48 < 27:01, 0.33 it/s, Epoch 2.58/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.018700</td>\n",
       "      <td>0.962100</td>\n",
       "      <td>1.053691</td>\n",
       "      <td>130373.000000</td>\n",
       "      <td>0.782306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.726700</td>\n",
       "      <td>0.746448</td>\n",
       "      <td>0.827214</td>\n",
       "      <td>250001.000000</td>\n",
       "      <td>0.829311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.496900</td>\n",
       "      <td>0.710210</td>\n",
       "      <td>0.657830</td>\n",
       "      <td>373966.000000</td>\n",
       "      <td>0.841228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.427600</td>\n",
       "      <td>0.623209</td>\n",
       "      <td>0.592642</td>\n",
       "      <td>501593.000000</td>\n",
       "      <td>0.858397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.386600</td>\n",
       "      <td>0.641123</td>\n",
       "      <td>0.535451</td>\n",
       "      <td>623666.000000</td>\n",
       "      <td>0.859420</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[9]\u001B[39m\u001B[32m, line 56\u001B[39m\n\u001B[32m     53\u001B[39m     trainer.accelerator.scaler = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m     54\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33m[patch] GradScaler disabled\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m56\u001B[39m \u001B[43mtrainer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     58\u001B[39m trainer.model.save_pretrained(OUT_DIR)\n\u001B[32m     59\u001B[39m tokenizer.save_pretrained(OUT_DIR)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/transformers/trainer.py:2325\u001B[39m, in \u001B[36mTrainer.train\u001B[39m\u001B[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[39m\n\u001B[32m   2323\u001B[39m         hf_hub_utils.enable_progress_bars()\n\u001B[32m   2324\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m2325\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2326\u001B[39m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[43m=\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2327\u001B[39m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m=\u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2328\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2329\u001B[39m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m=\u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2330\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/transformers/trainer.py:2674\u001B[39m, in \u001B[36mTrainer._inner_training_loop\u001B[39m\u001B[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[39m\n\u001B[32m   2667\u001B[39m context = (\n\u001B[32m   2668\u001B[39m     functools.partial(\u001B[38;5;28mself\u001B[39m.accelerator.no_sync, model=model)\n\u001B[32m   2669\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m i != \u001B[38;5;28mlen\u001B[39m(batch_samples) - \u001B[32m1\u001B[39m\n\u001B[32m   2670\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001B[32m   2671\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m contextlib.nullcontext\n\u001B[32m   2672\u001B[39m )\n\u001B[32m   2673\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m context():\n\u001B[32m-> \u001B[39m\u001B[32m2674\u001B[39m     tr_loss_step = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_items_in_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2676\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m   2677\u001B[39m     args.logging_nan_inf_filter\n\u001B[32m   2678\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_xla_available()\n\u001B[32m   2679\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m (torch.isnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch.isinf(tr_loss_step))\n\u001B[32m   2680\u001B[39m ):\n\u001B[32m   2681\u001B[39m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[32m   2682\u001B[39m     tr_loss = tr_loss + tr_loss / (\u001B[32m1\u001B[39m + \u001B[38;5;28mself\u001B[39m.state.global_step - \u001B[38;5;28mself\u001B[39m._globalstep_last_logged)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:1245\u001B[39m, in \u001B[36mSFTTrainer.training_step\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1243\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mtraining_step\u001B[39m(\u001B[38;5;28mself\u001B[39m, *args, **kwargs):\n\u001B[32m   1244\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m.maybe_activation_offload_context:\n\u001B[32m-> \u001B[39m\u001B[32m1245\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/transformers/trainer.py:4071\u001B[39m, in \u001B[36mTrainer.training_step\u001B[39m\u001B[34m(***failed resolving arguments***)\u001B[39m\n\u001B[32m   4068\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.accelerator.distributed_type == DistributedType.DEEPSPEED:\n\u001B[32m   4069\u001B[39m         kwargs[\u001B[33m\"\u001B[39m\u001B[33mscale_wrt_gas\u001B[39m\u001B[33m\"\u001B[39m] = \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m4071\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43maccelerator\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mloss\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   4073\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m loss.detach()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/accelerate/accelerator.py:2740\u001B[39m, in \u001B[36mAccelerator.backward\u001B[39m\u001B[34m(self, loss, **kwargs)\u001B[39m\n\u001B[32m   2738\u001B[39m     \u001B[38;5;28mself\u001B[39m.lomo_backward(loss, learning_rate)\n\u001B[32m   2739\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m2740\u001B[39m     \u001B[43mloss\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/torch/_tensor.py:625\u001B[39m, in \u001B[36mTensor.backward\u001B[39m\u001B[34m(self, gradient, retain_graph, create_graph, inputs)\u001B[39m\n\u001B[32m    615\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    616\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[32m    617\u001B[39m         Tensor.backward,\n\u001B[32m    618\u001B[39m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[32m   (...)\u001B[39m\u001B[32m    623\u001B[39m         inputs=inputs,\n\u001B[32m    624\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m625\u001B[39m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mautograd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    626\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs\u001B[49m\n\u001B[32m    627\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001B[39m, in \u001B[36mbackward\u001B[39m\u001B[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[39m\n\u001B[32m    349\u001B[39m     retain_graph = create_graph\n\u001B[32m    351\u001B[39m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[32m    352\u001B[39m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[32m    353\u001B[39m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m354\u001B[39m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    355\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    356\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    357\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    358\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    359\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs_tuple\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    360\u001B[39m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    361\u001B[39m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    362\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/torch/autograd/graph.py:841\u001B[39m, in \u001B[36m_engine_run_backward\u001B[39m\u001B[34m(t_outputs, *args, **kwargs)\u001B[39m\n\u001B[32m    839\u001B[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[32m    840\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m841\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_execution_engine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[32m    842\u001B[39m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    843\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[32m    844\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    845\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Быстрый тест",
   "id": "7a490b1865eaff63"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### выгрузка модели",
   "id": "465ce4242b71ec04"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T19:46:16.189028Z",
     "start_time": "2025-12-18T19:45:49.426371Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "from peft import PeftModel\n",
    "from llm_client import MODEL_NAME\n",
    "\n",
    "#ADAPTER_DIR = OUT_DIR  # или r\"RH-AI\\RH AI\\src\\QLoRA\\vac_qlora_adapter\"\n",
    "ADAPTER_DIR = r\"QLoRA/vac_qlora_adapter/checkpoint-200\"\n",
    "# 4-bit config (как в обучении)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# базовая модель целиком на GPU:0 (без CPU/disk dispatch)\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": 0},\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# подключаем QLoRA адаптер\n",
    "model = PeftModel.from_pretrained(base, ADAPTER_DIR)\n",
    "model.eval()\n",
    "\n",
    "# генератор\n",
    "gen_pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map={\"\": 0},\n",
    "    return_full_text=False,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    ")\n",
    "\n",
    "print(\"[OK] base+LoRA loaded, gen_pipe is ready\")\n"
   ],
   "id": "e8d35fdcb57a7d3b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "56ccb408ca814f08bfe345467283fde9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] base+LoRA loaded, gen_pipe is ready\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Сам тест",
   "id": "949232d34353f9a2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T19:47:06.653612Z",
     "start_time": "2025-12-18T19:46:59.141600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "TEST_PROMPT = \"\"\"Проанализируй текст вакансии и выдели ТОЛЬКО профессиональные компетенции из текста вакансии.\n",
    "Возможными компетенциями могут быть: языки программирования, фреймворки, библиотеки, базы данных и методы работы с ними, разработка чат-ботов, \n",
    "направления в индустриях, например построение ML-моделей, NLP, LLM, а также A/B-тестирование, геймдизайн, Power BI, разработка онлайн-курсов.\n",
    "\n",
    "Данные вакансии:\n",
    "Отрасль: 1CDevelopment\n",
    "Название: IT-специалист / Инженер 1 линии технической поддержки\n",
    "Описание: Описание вакансии\n",
    "Компания \"РОСТ\" - системный интегратор в области информационных технологий и Партнёр ведущих российских разработчиков Программного обеспечения, таких как: СБИС; 1С Bitrix; LiteBox и другие.\n",
    "Обязанности:\n",
    "• Принимать «горячие» обращения от клиентов\n",
    "• преимущественно звонки, а также мессенджеры;\n",
    "• правильно оформлять и заносить заявки в Help Desk;\n",
    "• удаленно оперативно устранять сбои в работе программно-аппаратного комплекса клиента\n",
    "• (система для автоматизации предприятий.);\n",
    "• правильно настраивать сеть, торговое оборудование и компьютерную технику;\n",
    "• качественно консультировать и обучать клиентов;\n",
    "• быть готовым работать с конфликтными клиентами;\n",
    "• уметь работать в команде.\n",
    "• выезд, если нет возможности решить проблему удаленно;\n",
    "• ведение и поддержание в актуальном состоянии документации;\n",
    "Требования:\n",
    "• опыт работы системным администратором или инженером технической поддержки (helpdesk/servicedesk) приветствуется;\n",
    "• опыт решения пользовательских задач (проблемы с офисным ПО, оргтехникой, работа с общими папками, сетевая печать, ЭЦП и т. п.) приветствуется;\n",
    "• опыт работы с антивирусным ПО (в т. ч. с централизованной консолью управления (в основном мы используем Kaspersky, Dr. Web);\n",
    "• плюсом будет наличие сертификатов (в области\n",
    "• ИТ\n",
    "• плюсом будет опыт администрирования 1С;\n",
    "• плюсом будет опыт работы в Битрикс24;\n",
    "...\n",
    "Формат ответа:\n",
    "[\"компетенция1\", \"компетенция2\", ...]\n",
    "Ответ:\"\"\"\n",
    "\n",
    "test2 = \"\"\"Проанализируй текст вакансии и выдели ТОЛЬКО профессиональные компетенции из текста вакансии.\n",
    "Возможными компетенциями могут быть: языки программирования, фреймворки, библиотеки, базы данных и методы работы с ними, разработка чат-ботов, \n",
    "направления в индустриях, например построение ML-моделей, NLP, LLM, а также A/B-тестирование, геймдизайн, Power BI, разработка онлайн-курсов.\n",
    "\n",
    "Данные вакансии:\n",
    "Отрасль: ChatBots\n",
    "Название: Продавец-кассир (ТЦ Макси Сопот, Приморская)\n",
    "Описание: Familia - основоположник и лидер российского off-price ритейла - благодаря своей товарной и ценовой политике, широте и частоте обновление ассортимента. В магазинах сети представлен широкий ассортимент мужской, женской и детской одежды и обуви, аксессуары, игрушки, весь спектр товаров для дома и декора, товары для домашних питомцев и многое другое по максимально выгодным ценам.\n",
    "Уважаемые кандидаты, мы ждём Вас на собеседование в магазине Familia со вторника по субботу с 10:00 до 19:00 БЕЗ ПРЕДВАРИТЕЛЬНОЙ ЗАПИСИ.\n",
    "По дополнительным вопросам, пожалуйста, звоните по указанному номеру телефона или пишите в чат, WhatsApp.\n",
    "Мы предлагаем:\n",
    "• Оформление по ТК РФ с первого дня работы, оплачиваемый больничный лист, отпуск.\n",
    "• Официальная белая заработная плата, выплачивается два раза в месяц.\n",
    "• Стабильный оклад + ежемесячная премия 15% по итогам работы магазина.\n",
    "• Отсутствие личных продаж!\n",
    "• График работы 2/2.\n",
    "• Предоставление форменной одежды.\n",
    "• Корпоративное обучение.\n",
    "• Перспектива карьерного роста до Заместителя управляющего магазином, Управляющего магазином.\n",
    "• Бонус по акции \"Приведи друга\" до 5000 руб.\n",
    "• Программа лояльности для сотрудников по системе кэшбек баллов.\n",
    "• Новогодние подарки для детей сотрудников.\n",
    "• Дополнительные конкурсные программы с денежным вознаграждением.\n",
    "• Бонусные программы для сотрудников (скидки от партнеров: красота, спорт, развлечения, электроника, туризм и многое другое).\n",
    "Основные задачи:\n",
    "• Работа за кассовым терминалом, обслуживание покупателей на кассе.\n",
    "• Поддержание порядка, соблюдение стандартов визуального мерчендайзинга и выкладка товара в прикассовой зоне\n",
    "• Участие в инвентаризации (1 раз в год).\n",
    "Для нас важно:\n",
    "• Активность и готовность обучаться.\n",
    "• Ответственность, внимательность, доброжелательность.\n",
    "• Имеете опыт работы – отлично! Без опыта – обучим!\n",
    "Приветствуется опыт работы на вакансиях: продавец, без опыта, менеджер по продажам, консультант, кассир, менеджер по работе с клиентами, начинающий специалист, ртз, работник торгового зала, кассир с ежедневными выплатами в компаниях:\n",
    "Gloria Jeans, Глория Джинс, Sela, Zarina, Зарина, Детский мир, Дочки-сыночки, Defacto, LC Waikiki, Sinsay, Sin, Синсей, Reserved, H&M, Cropp, House, Modis, Модис, Decatlon, Декатлон, MAAG, ECRU, DUB, VILET, Новая мода, Zara, Gloria-Jeans, GJ, Зара, Incity, Инсити, Inditex, Bershka, Massimo Dutti, Uterque, Pull&Bear, Stradivarius, Oysho, Lefties, Ойшо, Бершка, Пулл энд Беар, Массимо Дутти, Лефтиес, Kari, Кари, Intimissimi, Calzedonia, Кальзедония, Incanto, Calvin Klein, Collins, Коллинс, ТВОЕ, Levi’s, Mango, 12Storeez, Подружка, 5 карманов, Koton, Котон, Love Republic, New Yorker, Terranova, Терранова, Finn Flare, Фин Флэр, Zolla, Золла, Oodji, Оджи, Adidas, МВидео, Леруа Мерлен, OBI, SuperStep, Триал-Спорт, ZENDEN, KIABI, Киаби, RESPECT, TAMARIS, TERVOLINA, Хоум Маркет, Терволина, Nike, PUMA, Columbia, Modis, Demix, Mango, МВидео, Terranova, Эльдорадо, ИКЕА, Zolla, Oodj, Love Republic, Reebok, Связной, МТС, Мегафон, Билайн, Adidas, New Balance, Decathlon, Puma, Reebok, New Balance, Fun Day, Фандэй, Фандей, Sportmaster, Befre, Мохито, Mohito, Benetton, Intimissimi, Интимисими, Calzedonia, Калзедония, Incanto, Инканто, Incity, Инсити, Uniqlo, Юникло, Подружка, SuperStep, Zenden, Kiabi, Киаби, Respect, Rendez-vous, Рандеву, Терволина, МВидео, Эльдорадо, ИКЕА, Леруа Мерлен, OBI, Hoff, Хоф, Твой Дом, Галамарт, Хоум Маркет, DNS, Связной, МТС, Мегафон, Билайн, Вкусно и точка, Макдоналдс, Золотое яблоко, Idol, Yandex, KFC, Бургер Кинг, Burger King, Ростикс, Иль де боте, Fix price, Off price, Сбербанк, Тинькофф, Tinkoff, СДЭК, OZON, WILDBERRIES, Остин, Озон, Валдберрис, Kidzania, Кидзания, Совкомбанк, Альфа-Банк, Додо Пицца, Melon Fashion Group, Apple, Samsung, IL Патио, Шоколадница, Кофе Хаус, Cofix, Stars coffee, Surf Coffee, Gate31, 12 Cторис, Bell you, Джинсовая Симфония, Jeans Symphony, Ривгош, ИвРоше, Tous, Yves Rocher, One&Double, Double coffee, Star Hit, Nice Price, Bubble tea, Братья Караваевы, Гурманика, Азбука Вкуса, Вкусви\n",
    "\n",
    "Верни только JSON-массив строк. Компетенций(строк) в массиве должно быть не менее одной, но НЕ БОЛЕЕ 7. Компетенций должны быть выделены строго из текста вакансии.\n",
    "Не добавляй в массив строк компетенции, не относящиеся к вакансии.\n",
    "Формат ответа:\n",
    "[\"компетенция1\", \"компетенция2\", ...]\n",
    "Ответ:\"\"\"\n",
    "prompts = [\n",
    "    (\"1c test\", TEST_PROMPT),\n",
    "    (\"Продавец test\", test2),\n",
    "]\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    t = tokenizer(text, return_tensors=None, add_special_tokens=False)\n",
    "    return len(t[\"input_ids\"])\n",
    "\n",
    "print(\"Token counts:\")\n",
    "for name, p in prompts:\n",
    "    print(f\"{name}: {count_tokens(p)}\")\n",
    "\n",
    "print(\"\\n=== QLoRA QUICK TEST ===\")\n",
    "with torch.inference_mode():\n",
    "    for name, p in prompts:\n",
    "        out = gen_pipe(\n",
    "            p,\n",
    "            max_new_tokens=128,\n",
    "            min_new_tokens=2,\n",
    "            do_sample=True,\n",
    "            temperature=0.3,\n",
    "            top_p=0.9,\n",
    "        )[0][\"generated_text\"]\n",
    "\n",
    "        print(f\"\\n--- {name} ---\")\n",
    "        print(out)"
   ],
   "id": "60134d9332101c16",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token counts:\n",
      "1c test: 617\n",
      "Продавец test: 1817\n",
      "\n",
      "=== QLoRA QUICK TEST ===\n",
      "\n",
      "--- 1c test ---\n",
      " [\"телефонная поддержка\", \"работа с клиентами\", \"решение технических задач\", \"консультирование\", \"ведение документации\", \"работа с ПО\", \"анализ проблем\", \"поиск решений\", \"тестирование\"]\n",
      "\n",
      "--- Продавец test ---\n",
      " [\"работа с клиентами\", \"ведение продаж\", \"ведение кассовой операции\", \"обслуживание покупателей\", \"работа с товарным ассортиментом\", \"поддержание чистоты и порядка\", \"коммуникация с коллегами\"]\n"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
